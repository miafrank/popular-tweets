# Popular Tweets
Stream tweets by keyword using Kafka Producer, Kafka Stream, and Dockerize HBase instance for persistence. 

# Setup
### Project setup - Intellij 
1. From the main menu, select `File` | `Open`.
   1. Alternatively, click `Open` or `Import` on the welcome screen.
1. In the dialog that opens, select the `pom.xml` file of the project you want to open.
Click **`OK`**.
1. In the dialog that opens, click `Open as Project`.

IntelliJ IDEA opens and syncs the Maven project in the IDE. 
If you need to adjust importing options when you open the project, refer to the 
[Maven](https://www.jetbrains.com/help/idea/maven1.importing.html) settings.

### Additional Setup
1. [Confluent Platform](https://www.confluent.io/download/?_ga=2.250927620.387832776.1589147004-1583349907.1588185083) 5.2 or later
1. [Confluent CLI](https://docs.confluent.io/current/cli/installing.html#cli-install)
1. Java 1.8 or 1.11 to run Confluent Platform
    1. MacOS Java 8 installation: 
        ```bash
        brew tap adoptopenjdk/openjdk
        brew cask install adoptopenjdk8
        ```
1. [Maven](https://maven.apache.org/) to compile the client Java code (If using Intellij -- Maven comes bundled in the IDE, so you can skip this step)
1. [Docker](https://www.docker.com/get-started)
1. Apache HBase Sink Connector (writes data from a topic in Kafka to a table in the specified HBase instance)
    ```bash
    confluent-hub install confluentinc/kafka-connect-hbase:latest
    ```
### Twitter Developer Account
Apply for a [Twitter Developer Account](https://developer.twitter.com/en/docs/basics/developer-portal/faq) to receive 
access tokens and keys to use Twitter API. When the tokens are received, keys and tokens can be generated by creating an `App` in the Twitter
Developer dashboard.

To crate an app:  
    1. `Apps` -> `Create an app` -> Fill out App details form. 
    2. Copy key and tokens into `twitter.properties` file. 
    
### Kafka Producer
The producer ingests tweets from Twitter API configured by a list of search terms.
```java
public class TwitterProducer {
    List<String> terms = Lists.newArrayList("conspiracy", "conspiracyTheory", "fakenews");
}
```
All messages conform to a certain schema (class) defined in `/resources/avro/Tweets.avsc` before they are sent to Kafka topic in Avro format. 
```java
  public Tweets(java.lang.CharSequence tweet, java.lang.CharSequence userName, java.lang.Integer userNumFollowers) {
    this.tweet = tweet;
    this.userName = userName;
    this.userNumFollowers = userNumFollowers;
  }
```
To create the code-generated class, compile the Java class from the Tweets.avsc file: 
```bash
mvn clean compile package
```

### Kafka Stream
The stream ingests messages from the Kafka topic and filters on user's number of followers. After filtering, 
the messages are persisted in a new Kafka Topic. 

### Dockerized Hbase Instance (Using Confluent HBase Sink Connector)
A single configuration file `hbase-avro.json` is used to configure which topic the Hbase instance should 
subscribe to (filtered kafka topic) in order to persist messages to an HBase table.